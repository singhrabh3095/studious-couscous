{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CRNP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWiRafr193l8",
        "outputId": "86be602d-9ed0-4901-eb25-572a0955fc72"
      },
      "source": [
        "import argparse\n",
        "import datetime\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "from sklearn.preprocessing import scale\n",
        "from timeit import default_timer as timer\n",
        "import sys\n",
        "sys.argv=['']\n",
        "del sys\n",
        "\n",
        "\n",
        "### global constant variables\n",
        "#------------------------------------------------------------------------\n",
        "# general\n",
        "#------------------------------------------------------------------------\n",
        "INPUT_DIM = 520                 #  number of APs\n",
        "VERBOSE = 1                     # 0 for turning off logging\n",
        "#------------------------------------------------------------------------\n",
        "# stacked auto encoder (sae)\n",
        "#------------------------------------------------------------------------\n",
        "# SAE_ACTIVATION = 'tanh'\n",
        "SAE_ACTIVATION = 'relu'\n",
        "SAE_BIAS = False\n",
        "SAE_OPTIMIZER = 'adam'\n",
        "SAE_LOSS = 'mse'\n",
        "#------------------------------------------------------------------------\n",
        "# classifier\n",
        "#------------------------------------------------------------------------\n",
        "CLASSIFIER_ACTIVATION = 'relu'\n",
        "CLASSIFIER_BIAS = False\n",
        "CLASSIFIER_OPTIMIZER = 'adam'\n",
        "CLASSIFIER_LOSS = 'binary_crossentropy'\n",
        "#------------------------------------------------------------------------\n",
        "# input files\n",
        "#------------------------------------------------------------------------\n",
        "path_train = \"/content/trainingData.csv\"           # '-110' for the lack of AP.\n",
        "path_validation = \"/content/validationData.csv\"    # ditto\n",
        "#------------------------------------------------------------------------\n",
        "# output files\n",
        "#------------------------------------------------------------------------\n",
        "path_base = '../results/' + os.path.splitext(os.path.basename('__file__'))[0]\n",
        "path_out =  path_base + '_out'\n",
        "path_sae_model = path_base + '_sae_model.hdf5'\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        \"-G\",\n",
        "        \"--gpu_id\",\n",
        "        help=\"ID of GPU device to run this script; default is 0; set it to a negative number for CPU (i.e., no GPU)\",\n",
        "        default=0,\n",
        "        type=int)\n",
        "    parser.add_argument(\n",
        "        \"-R\",\n",
        "        \"--random_seed\",\n",
        "        help=\"random seed\",\n",
        "        default=0,\n",
        "        type=int)\n",
        "    parser.add_argument(\n",
        "        \"-E\",\n",
        "        \"--epochs\",\n",
        "        help=\"number of epochs; default is 20\",\n",
        "        default=20,\n",
        "        type=int)\n",
        "    parser.add_argument(\n",
        "        \"-B\",\n",
        "        \"--batch_size\",\n",
        "        help=\"batch size; default is 10\",\n",
        "        default=10,\n",
        "        type=int)\n",
        "    parser.add_argument(\n",
        "        \"-T\",\n",
        "        \"--training_ratio\",\n",
        "        help=\"ratio of training data to overall data: default is 0.9\",\n",
        "        default=0.9,\n",
        "        type=float)\n",
        "    parser.add_argument(\n",
        "        \"-S\",\n",
        "        \"--sae_hidden_layers\",\n",
        "        help=\n",
        "        \"comma-separated numbers of units in SAE hidden layers; default is '256,128,64,128,256'\",\n",
        "        default='256,128,64,128,256',\n",
        "        type=str)\n",
        "    parser.add_argument(\n",
        "        \"-C\",\n",
        "        \"--classifier_hidden_layers\",\n",
        "        help=\n",
        "        \"comma-separated numbers of units in classifier hidden layers; default is '128,128'\",\n",
        "        default='128,128',\n",
        "        type=str)\n",
        "    parser.add_argument(\n",
        "        \"-D\",\n",
        "        \"--dropout\",\n",
        "        help=\n",
        "        \"dropout rate before and after classifier hidden layers; default 0.0\",\n",
        "        default=0.0,\n",
        "        type=float)\n",
        "    # parser.add_argument(\n",
        "    #     \"--building_weight\",\n",
        "    #     help=\n",
        "    #     \"weight for building classes in classifier; default 1.0\",\n",
        "    #     default=1.0,\n",
        "    #     type=float)\n",
        "    # parser.add_argument(\n",
        "    #     \"--floor_weight\",\n",
        "    #     help=\n",
        "    #     \"weight for floor classes in classifier; default 1.0\",\n",
        "    #     default=1.0,\n",
        "    #     type=float)\n",
        "    parser.add_argument(\n",
        "        \"-N\",\n",
        "        \"--neighbours\",\n",
        "        help=\"number of (nearest) neighbour locations to consider in positioning; default is 1\",\n",
        "        default=1,\n",
        "        type=int)\n",
        "    parser.add_argument(\n",
        "        \"--scaling\",\n",
        "        help=\n",
        "        \"scaling factor for threshold (i.e., threshold=scaling*maximum) for the inclusion of nighbour locations to consider in positioning; default is 0.0\",\n",
        "        default=0.0,\n",
        "        type=float)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # set variables using command-line arguments\n",
        "    gpu_id = args.gpu_id\n",
        "    random_seed = args.random_seed\n",
        "    epochs = args.epochs\n",
        "    batch_size = args.batch_size\n",
        "    training_ratio = args.training_ratio\n",
        "    sae_hidden_layers = [int(i) for i in (args.sae_hidden_layers).split(',')]\n",
        "    if args.classifier_hidden_layers == '':\n",
        "        classifier_hidden_layers = ''\n",
        "    else:\n",
        "        classifier_hidden_layers = [int(i) for i in (args.classifier_hidden_layers).split(',')]\n",
        "    dropout = args.dropout\n",
        "    # building_weight = args.building_weight\n",
        "    # floor_weight = args.floor_weight\n",
        "    N = args.neighbours\n",
        "    scaling = args.scaling\n",
        "\n",
        "    ### initialize random seed generator of numpy\n",
        "    np.random.seed(random_seed)\n",
        "    \n",
        "    #--------------------------------------------------------------------\n",
        "    # import keras and its backend (e.g., tensorflow)\n",
        "    #--------------------------------------------------------------------\n",
        "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
        "    if gpu_id >= 0:\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
        "    else:\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = ''\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL']='2'  # supress warning messages\n",
        "    import tensorflow as tf\n",
        "    #tf.set_random_seed(random_seed)  # initialize random seed generator of tensorflow\n",
        "    tf.random.set_seed(random_seed)\n",
        "    from keras.layers import Dense, Dropout\n",
        "    from keras.models import Sequential, load_model\n",
        "\n",
        "    # read both train and test dataframes for consistent label formation through one-hot encoding\n",
        "    train_df = pd.read_csv(path_train, header=0) # pass header=0 to be able to replace existing names\n",
        "    test_df = pd.read_csv(path_validation, header=0)\n",
        "    \n",
        "    train_AP_features = scale(np.asarray(train_df.iloc[:,0:520]).astype(float), axis=1) # convert integer to float and scale jointly (axis=1)\n",
        "    train_df['REFPOINT'] = train_df.apply(lambda row: str(int(row['SPACEID'])) + str(int(row['RELATIVEPOSITION'])), axis=1) # add a new column\n",
        "    \n",
        "    blds = np.unique(train_df[['BUILDINGID']])\n",
        "    flrs = np.unique(train_df[['FLOOR']])\n",
        "    x_avg = {}\n",
        "    y_avg = {}\n",
        "    for bld in blds:\n",
        "        for flr in flrs:\n",
        "            # map reference points to sequential IDs per building-floor before building labels\n",
        "            cond = (train_df['BUILDINGID']==bld) & (train_df['FLOOR']==flr)\n",
        "            _, idx = np.unique(train_df.loc[cond, 'REFPOINT'], return_inverse=True) # refer to numpy.unique manual\n",
        "            train_df.loc[cond, 'REFPOINT'] = idx\n",
        "            \n",
        "            # calculate the average coordinates of each building/floor\n",
        "            x_avg[str(bld) + '-' + str(flr)] = np.mean(train_df.loc[cond, 'LONGITUDE'])\n",
        "            y_avg[str(bld) + '-' + str(flr)] = np.mean(train_df.loc[cond, 'LATITUDE'])\n",
        "    \n",
        "    # build labels for multi-label classification\n",
        "    len_train = len(train_df)\n",
        "    blds_all = np.asarray(pd.get_dummies(pd.concat([train_df['BUILDINGID'], test_df['BUILDINGID']]))) # for consistency in one-hot encoding for both dataframes\n",
        "    flrs_all = np.asarray(pd.get_dummies(pd.concat([train_df['FLOOR'], test_df['FLOOR']]))) # ditto\n",
        "    blds = blds_all[:len_train]\n",
        "    flrs = flrs_all[:len_train]\n",
        "    # blds = np.asarray(pd.get_dummies(train_df['BUILDINGID']))\n",
        "    # flrs = np.asarray(pd.get_dummies(train_df['FLOOR']))\n",
        "    rfps = np.asarray(pd.get_dummies(train_df['REFPOINT']))\n",
        "    train_labels = np.concatenate((blds, flrs, rfps), axis=1)\n",
        "    # labels is an array of 19937 x 118\n",
        "    # - 3 for BUILDINGID\n",
        "    # - 5 for FLOOR,\n",
        "    # - 110 for REFPOINT\n",
        "    OUTPUT_DIM = train_labels.shape[1]\n",
        "    \n",
        "    # split the training set into training and validation sets; we will use the\n",
        "    # validation set at a testing set.\n",
        "    train_val_split = np.random.rand(len(train_AP_features)) < training_ratio # mask index array\n",
        "    x_train = train_AP_features[train_val_split]\n",
        "    y_train = train_labels[train_val_split]\n",
        "    x_val = train_AP_features[~train_val_split]\n",
        "    y_val = train_labels[~train_val_split]\n",
        "\n",
        "    ### build SAE encoder model\n",
        "    print(\"\\nPart 1: buidling an SAE encoder ...\")\n",
        "    if False:\n",
        "    # if os.path.isfile(path_sae_model) and (os.path.getmtime(path_sae_model) > os.path.getmtime(__file__)):\n",
        "        model = load_model(path_sae_model)\n",
        "    else:\n",
        "        # create a model based on stacked autoencoder (SAE)\n",
        "        model = Sequential()\n",
        "        model.add(Dense(sae_hidden_layers[0], input_dim=INPUT_DIM, activation=SAE_ACTIVATION, use_bias=SAE_BIAS))\n",
        "        for units in sae_hidden_layers[1:]:\n",
        "            model.add(Dense(units, activation=SAE_ACTIVATION, use_bias=SAE_BIAS))  \n",
        "        model.add(Dense(INPUT_DIM, activation=SAE_ACTIVATION, use_bias=SAE_BIAS))\n",
        "        model.compile(optimizer=SAE_OPTIMIZER, loss=SAE_LOSS)\n",
        "\n",
        "        # train the model\n",
        "        model.fit(x_train, x_train, batch_size=batch_size, epochs=epochs, verbose=VERBOSE)\n",
        "\n",
        "        # remove the decoder part\n",
        "        num_to_remove = (len(sae_hidden_layers) + 1) // 2\n",
        "        for i in range(num_to_remove):\n",
        "            model.pop()\n",
        "\n",
        "        # # set all layers (i.e., SAE encoder) to non-trainable (weights will not be updated)\n",
        "        # for layer in model.layers[:]:\n",
        "        #     layer.trainable = False\n",
        "        \n",
        "        # # save the model for later use\n",
        "        # model.save(path_sae_model)\n",
        "\n",
        "    ### build and train a complete model with the trained SAE encoder and a new classifier\n",
        "    print(\"\\nPart 2: buidling a complete model ...\")\n",
        "    # append a classifier to the model\n",
        "    # class_weight = {\n",
        "    #     0: building_weight, 1: building_weight, 2: building_weight,  # buildings\n",
        "    #     3: floor_weight, 4: floor_weight, 5: floor_weight, 6:floor_weight, 7: floor_weight  # floors\n",
        "    # }\n",
        "    model.add(Dropout(dropout))\n",
        "    for units in classifier_hidden_layers:\n",
        "        model.add(Dense(units, activation=CLASSIFIER_ACTIVATION, use_bias=CLASSIFIER_BIAS))\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(Dense(OUTPUT_DIM, activation='sigmoid', use_bias=CLASSIFIER_BIAS)) # 'sigmoid' for multi-label classification\n",
        "    model.compile(optimizer=CLASSIFIER_OPTIMIZER, loss=CLASSIFIER_LOSS, metrics=['accuracy'])\n",
        "\n",
        "    # train the model\n",
        "    startTime = timer()\n",
        "    model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=batch_size, epochs=epochs, verbose=VERBOSE)\n",
        "    # model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=batch_size, epochs=epochs, class_weight=class_weight, verbose=VERBOSE)\n",
        "    elapsedTime = timer() - startTime\n",
        "    print(\"Model trained in %e s.\" % elapsedTime)\n",
        "    \n",
        "    # turn the given validation set into a testing set\n",
        "    # test_df = pd.read_csv(path_validation, header=0)\n",
        "    test_AP_features = scale(np.asarray(test_df.iloc[:,0:520]).astype(float), axis=1) # convert integer to float and scale jointly (axis=1)\n",
        "    x_test_utm = np.asarray(test_df['LONGITUDE'])\n",
        "    y_test_utm = np.asarray(test_df['LATITUDE'])\n",
        "    # blds = np.asarray(pd.get_dummies(test_df['BUILDINGID']))\n",
        "    blds = blds_all[len_train:]\n",
        "    # flrs = np.asarray(pd.get_dummies(test_df['FLOOR']))\n",
        "    flrs = flrs_all[len_train:]\n",
        "\n",
        "    ### evaluate the model\n",
        "    print(\"\\nPart 3: evaluating the model ...\")\n",
        "    \n",
        "    # calculate the accuracy of building and floor estimation\n",
        "    preds = model.predict(test_AP_features, batch_size=batch_size)\n",
        "    n_preds = preds.shape[0]\n",
        "    # blds_results = (np.equal(np.argmax(test_labels[:, :3], axis=1), np.argmax(preds[:, :3], axis=1))).astype(int)\n",
        "    blds_results = (np.equal(np.argmax(blds, axis=1), np.argmax(preds[:, :3], axis=1))).astype(int)\n",
        "    acc_bld = blds_results.mean()\n",
        "    # flrs_results = (np.equal(np.argmax(test_labels[:, 3:8], axis=1), np.argmax(preds[:, 3:8], axis=1))).astype(int)\n",
        "    flrs_results = (np.equal(np.argmax(flrs, axis=1), np.argmax(preds[:, 3:8], axis=1))).astype(int)\n",
        "    acc_flr = flrs_results.mean()\n",
        "    acc_bf = (blds_results*flrs_results).mean()\n",
        "    # rfps_results = (np.equal(np.argmax(test_labels[:, 8:118], axis=1), np.argmax(preds[:, 8:118], axis=1))).astype(int)\n",
        "    # acc_rfp = rfps_results.mean()\n",
        "    # acc = (blds_results*flrs_results*rfps_results).mean()\n",
        "    \n",
        "    # calculate positioning error when building and floor are correctly estimated\n",
        "    mask = np.logical_and(blds_results, flrs_results) # mask index array for correct location of building and floor\n",
        "    x_test_utm = x_test_utm[mask]\n",
        "    y_test_utm = y_test_utm[mask]\n",
        "    blds = blds[mask]\n",
        "    flrs = flrs[mask]\n",
        "    rfps = (preds[mask])[:, 8:118]\n",
        "\n",
        "    n_success = len(blds)       # number of correct building and floor location\n",
        "    # blds = np.greater_equal(blds, np.tile(np.amax(blds, axis=1).reshape(n_success, 1), (1, 3))).astype(int) # set maximum column to 1 and others to 0 (row-wise)\n",
        "    # flrs = np.greater_equal(flrs, np.tile(np.amax(flrs, axis=1).reshape(n_success, 1), (1, 5))).astype(int) # ditto\n",
        "\n",
        "    n_loc_failure = 0\n",
        "    sum_pos_err = 0.0\n",
        "    sum_pos_err_weighted = 0.0\n",
        "    idxs = np.argpartition(rfps, -N)[:, -N:]  # (unsorted) indexes of up to N nearest neighbors\n",
        "    threshold = scaling*np.amax(rfps, axis=1)\n",
        "    for i in range(n_success):\n",
        "        xs = []\n",
        "        ys = []\n",
        "        ws = []\n",
        "        for j in idxs[i]:\n",
        "            rfp = np.zeros(110)\n",
        "            rfp[j] = 1\n",
        "            rows = np.where((train_labels == np.concatenate((blds[i], flrs[i], rfp))).all(axis=1)) # tuple of row indexes\n",
        "            if rows[0].size > 0:\n",
        "                if rfps[i][j] >= threshold[i]:\n",
        "                    xs.append(train_df.loc[train_df.index[rows[0][0]], 'LONGITUDE'])\n",
        "                    ys.append(train_df.loc[train_df.index[rows[0][0]], 'LATITUDE'])\n",
        "                    ws.append(rfps[i][j])\n",
        "        if len(xs) > 0:\n",
        "            sum_pos_err += math.sqrt((np.mean(xs)-x_test_utm[i])**2 + (np.mean(ys)-y_test_utm[i])**2)\n",
        "            sum_pos_err_weighted += math.sqrt((np.average(xs, weights=ws)-x_test_utm[i])**2 + (np.average(ys, weights=ws)-y_test_utm[i])**2)\n",
        "        else:\n",
        "            n_loc_failure += 1\n",
        "            key = str(np.argmax(blds[i])) + '-' + str(np.argmax(flrs[i]))\n",
        "            pos_err = math.sqrt((x_avg[key]-x_test_utm[i])**2 + (y_avg[key]-y_test_utm[i])**2)\n",
        "            sum_pos_err += pos_err\n",
        "            sum_pos_err_weighted += pos_err\n",
        "    # mean_pos_err = sum_pos_err / (n_success - n_loc_failure)\n",
        "    mean_pos_err = sum_pos_err / n_success\n",
        "    # mean_pos_err_weighted = sum_pos_err_weighted / (n_success - n_loc_failure)\n",
        "    mean_pos_err_weighted = sum_pos_err_weighted / n_success\n",
        "    loc_failure = n_loc_failure / n_success # rate of location estimation failure given that building and floor are correctly located\n",
        "\n",
        "    ### print out final results\n",
        "    now = datetime.datetime.now()\n",
        "    path_out = \"_\" + now.strftime(\"%Y%m%d-%H%M%S\") + \".org\"\n",
        "    f = open(path_out, 'w')\n",
        "    f.write(\"#+STARTUP: showall\\n\")  # unfold everything when opening\n",
        "    f.write(\"* System parameters\\n\")\n",
        "    f.write(\"  - Numpy random number seed: %d\\n\" % random_seed)\n",
        "    f.write(\"  - Ratio of training data to overall data: %.2f\\n\" % training_ratio)\n",
        "    f.write(\"  - Number of epochs: %d\\n\" % epochs)\n",
        "    f.write(\"  - Batch size: %d\\n\" % batch_size)\n",
        "    f.write(\"  - Number of neighbours: %d\\n\" % N)\n",
        "    f.write(\"  - Scaling factor for threshold: %.2f\\n\" % scaling)\n",
        "    f.write(\"  - SAE hidden layers: %d\" % sae_hidden_layers[0])\n",
        "    for units in sae_hidden_layers[1:]:\n",
        "        f.write(\"-%d\" % units)\n",
        "    f.write(\"\\n\")\n",
        "    f.write(\"  - SAE activation: %s\\n\" % SAE_ACTIVATION)\n",
        "    f.write(\"  - SAE bias: %s\\n\" % SAE_BIAS)\n",
        "    f.write(\"  - SAE optimizer: %s\\n\" % SAE_OPTIMIZER)\n",
        "    f.write(\"  - SAE loss: %s\\n\" % SAE_LOSS)\n",
        "    f.write(\"  - Classifier hidden layers: \")\n",
        "    if classifier_hidden_layers == '':\n",
        "        f.write(\"N/A\\n\")\n",
        "    else:\n",
        "        f.write(\"%d\" % classifier_hidden_layers[0])\n",
        "        for units in classifier_hidden_layers[1:]:\n",
        "            f.write(\"-%d\" % units)\n",
        "        f.write(\"\\n\")\n",
        "        f.write(\"  - Classifier hidden layer activation: %s\\n\" % CLASSIFIER_ACTIVATION)\n",
        "    f.write(\"  - Classifier bias: %s\\n\" % CLASSIFIER_BIAS)\n",
        "    f.write(\"  - Classifier optimizer: %s\\n\" % CLASSIFIER_OPTIMIZER)\n",
        "    f.write(\"  - Classifier loss: %s\\n\" % CLASSIFIER_LOSS)\n",
        "    f.write(\"  - Classifier dropout rate: %.2f\\n\" % dropout)\n",
        "    # f.write(\"  - Classifier class weight for buildings: %.2f\\n\" % building_weight)\n",
        "    # f.write(\"  - Classifier class weight for floors: %.2f\\n\" % floor_weight)\n",
        "    f.write(\"* Performance\\n\")\n",
        "    f.write(\"  - Accuracy (building): %e\\n\" % acc_bld)\n",
        "    f.write(\"  - Accuracy (floor): %e\\n\" % acc_flr)\n",
        "    f.write(\"  - Accuracy (building-floor): %e\\n\" % acc_bf)\n",
        "    f.write(\"  - Location estimation failure rate (given the correct building/floor): %e\\n\" % loc_failure)\n",
        "    f.write(\"  - Positioning error (meter): %e\\n\" % mean_pos_err)\n",
        "    f.write(\"  - Positioning error (weighted; meter): %e\\n\" % mean_pos_err_weighted)\n",
        "    f.close()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Part 1: buidling an SAE encoder ...\n",
            "Epoch 1/20\n",
            "1793/1793 [==============================] - 8s 4ms/step - loss: 0.9667\n",
            "Epoch 2/20\n",
            "1793/1793 [==============================] - 8s 4ms/step - loss: 0.9649\n",
            "Epoch 3/20\n",
            "1793/1793 [==============================] - 7s 4ms/step - loss: 0.9646\n",
            "Epoch 4/20\n",
            "1793/1793 [==============================] - 7s 4ms/step - loss: 0.9644\n",
            "Epoch 5/20\n",
            "1793/1793 [==============================] - 7s 4ms/step - loss: 0.9642\n",
            "Epoch 6/20\n",
            "1793/1793 [==============================] - 7s 4ms/step - loss: 0.9641\n",
            "Epoch 7/20\n",
            "1793/1793 [==============================] - 7s 4ms/step - loss: 0.9640\n",
            "Epoch 8/20\n",
            "1793/1793 [==============================] - 7s 4ms/step - loss: 0.9640\n",
            "Epoch 9/20\n",
            "1793/1793 [==============================] - 7s 4ms/step - loss: 0.9639\n",
            "Epoch 10/20\n",
            "1793/1793 [==============================] - 7s 4ms/step - loss: 0.9638\n",
            "Epoch 11/20\n",
            "1793/1793 [==============================] - 7s 4ms/step - loss: 0.9638\n",
            "Epoch 12/20\n",
            "1793/1793 [==============================] - 7s 4ms/step - loss: 0.9638\n",
            "Epoch 13/20\n",
            "1793/1793 [==============================] - 7s 4ms/step - loss: 0.9637\n",
            "Epoch 14/20\n",
            "1793/1793 [==============================] - 7s 4ms/step - loss: 0.9637\n",
            "Epoch 15/20\n",
            "1793/1793 [==============================] - 7s 4ms/step - loss: 0.9637\n",
            "Epoch 16/20\n",
            "1793/1793 [==============================] - 7s 4ms/step - loss: 0.9636\n",
            "Epoch 17/20\n",
            "1793/1793 [==============================] - 8s 4ms/step - loss: 0.9636\n",
            "Epoch 18/20\n",
            "1793/1793 [==============================] - 8s 4ms/step - loss: 0.9636\n",
            "Epoch 19/20\n",
            "1793/1793 [==============================] - 7s 4ms/step - loss: 0.9636\n",
            "Epoch 20/20\n",
            "1793/1793 [==============================] - 7s 4ms/step - loss: 0.9635\n",
            "\n",
            "Part 2: buidling a complete model ...\n",
            "Epoch 1/20\n",
            "1793/1793 [==============================] - 6s 3ms/step - loss: 0.0578 - accuracy: 0.9182 - val_loss: 0.0412 - val_accuracy: 0.9099\n",
            "Epoch 2/20\n",
            "1793/1793 [==============================] - 6s 3ms/step - loss: 0.0371 - accuracy: 0.8911 - val_loss: 0.0342 - val_accuracy: 0.9199\n",
            "Epoch 3/20\n",
            "1793/1793 [==============================] - 6s 4ms/step - loss: 0.0323 - accuracy: 0.8894 - val_loss: 0.0320 - val_accuracy: 0.8771\n",
            "Epoch 4/20\n",
            "1793/1793 [==============================] - 6s 4ms/step - loss: 0.0290 - accuracy: 0.8814 - val_loss: 0.0302 - val_accuracy: 0.8661\n",
            "Epoch 5/20\n",
            "1793/1793 [==============================] - 6s 3ms/step - loss: 0.0269 - accuracy: 0.8830 - val_loss: 0.0285 - val_accuracy: 0.8985\n",
            "Epoch 6/20\n",
            "1793/1793 [==============================] - 6s 4ms/step - loss: 0.0250 - accuracy: 0.8725 - val_loss: 0.0270 - val_accuracy: 0.9044\n",
            "Epoch 7/20\n",
            "1793/1793 [==============================] - 6s 4ms/step - loss: 0.0236 - accuracy: 0.8806 - val_loss: 0.0259 - val_accuracy: 0.8561\n",
            "Epoch 8/20\n",
            "1793/1793 [==============================] - 6s 4ms/step - loss: 0.0223 - accuracy: 0.8729 - val_loss: 0.0267 - val_accuracy: 0.8726\n",
            "Epoch 9/20\n",
            "1793/1793 [==============================] - 6s 3ms/step - loss: 0.0212 - accuracy: 0.8660 - val_loss: 0.0263 - val_accuracy: 0.8556\n",
            "Epoch 10/20\n",
            "1793/1793 [==============================] - 6s 3ms/step - loss: 0.0203 - accuracy: 0.8561 - val_loss: 0.0263 - val_accuracy: 0.9064\n",
            "Epoch 11/20\n",
            "1793/1793 [==============================] - 6s 4ms/step - loss: 0.0196 - accuracy: 0.8652 - val_loss: 0.0255 - val_accuracy: 0.8711\n",
            "Epoch 12/20\n",
            "1793/1793 [==============================] - 6s 4ms/step - loss: 0.0189 - accuracy: 0.8533 - val_loss: 0.0261 - val_accuracy: 0.8641\n",
            "Epoch 13/20\n",
            "1793/1793 [==============================] - 7s 4ms/step - loss: 0.0180 - accuracy: 0.8439 - val_loss: 0.0266 - val_accuracy: 0.8492\n",
            "Epoch 14/20\n",
            "1793/1793 [==============================] - 7s 4ms/step - loss: 0.0177 - accuracy: 0.8553 - val_loss: 0.0269 - val_accuracy: 0.8726\n",
            "Epoch 15/20\n",
            "1793/1793 [==============================] - 7s 4ms/step - loss: 0.0172 - accuracy: 0.8454 - val_loss: 0.0265 - val_accuracy: 0.8447\n",
            "Epoch 16/20\n",
            "1793/1793 [==============================] - 7s 4ms/step - loss: 0.0167 - accuracy: 0.8426 - val_loss: 0.0266 - val_accuracy: 0.7899\n",
            "Epoch 17/20\n",
            "1793/1793 [==============================] - 7s 4ms/step - loss: 0.0162 - accuracy: 0.8407 - val_loss: 0.0270 - val_accuracy: 0.8303\n",
            "Epoch 18/20\n",
            "1793/1793 [==============================] - 7s 4ms/step - loss: 0.0158 - accuracy: 0.8277 - val_loss: 0.0271 - val_accuracy: 0.8228\n",
            "Epoch 19/20\n",
            "1793/1793 [==============================] - 6s 4ms/step - loss: 0.0155 - accuracy: 0.8189 - val_loss: 0.0266 - val_accuracy: 0.7999\n",
            "Epoch 20/20\n",
            "1793/1793 [==============================] - 6s 3ms/step - loss: 0.0152 - accuracy: 0.8323 - val_loss: 0.0275 - val_accuracy: 0.8547\n",
            "Model trained in 1.423766e+02 s.\n",
            "\n",
            "Part 3: evaluating the model ...\n"
          ]
        }
      ]
    }
  ]
}